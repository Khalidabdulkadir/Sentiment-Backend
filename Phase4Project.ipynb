{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be3d10c8-29cf-46f9-8420-c6464203269d",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Business Understanding\n",
    "\n",
    "### 1.1 Overview\n",
    "\n",
    "In the modern digital era, social media platforms like Twitter (now X) have become powerful channels for consumers to express opinions, experiences, and emotions about brands, products, and services. These views can significantly influence purchasing decisions, brand reputation, and marketing strategies.\n",
    "\n",
    "Manually tracking and interpreting this vast, unstructured feedback is impractical for companies. As a result, organizations increasingly turn to **Natural Language Processing (NLP)** and **machine learning models** to automatically analyze and interpret tweet sentiments.\n",
    "\n",
    "![NLP Diagram](nlp.image.png)\n",
    "*Figure 1: Illustration of how Natural Language Processing (NLP) processes and classifies text to produce outputs.*\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Business Problem\n",
    "\n",
    "Businesses need to understand **how customers feel** about their products and brands in real time. However, the sheer volume and unstructured nature of tweets make manual analysis impossible.\n",
    "\n",
    "The core challenge is to **automatically classify each tweet** as **positive**, **negative**, or **neutral**. This provides actionable insights to:\n",
    "\n",
    "- Identify emerging trends in customer satisfaction or dissatisfaction.\n",
    "- Track public reactions to product launches or campaigns.\n",
    "- Inform data-driven marketing and customer engagement decisions.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Project Objective\n",
    "\n",
    "**Main Objective:**  \n",
    "To **develop an automated sentiment classification model** that accurately analyzes and categorizes sentiments expressed in posts on **X (formerly Twitter)** as **positive, negative, or neutral**, enabling real-time insights into customer perceptions of a brand to support **data-driven marketing** and **brand management decisions**.\n",
    "\n",
    "**Specific Objectives:**  \n",
    "\n",
    "1. **Build a Binary Classification Model:**  \n",
    "   Develop and train a machine learning model to accurately distinguish between **positive** and **negative** sentiments in X posts.  \n",
    "\n",
    "2. **Extend to Multiclass Classification:**  \n",
    "   Enhance the model to classify posts into **four categories**:  \n",
    "   - No emotion toward brand or product  \n",
    "   - Positive emotion  \n",
    "   - Negative emotion  \n",
    "   - I can't tell  \n",
    "   This should be done **while maintaining or improving overall classification performance**.\n",
    "\n",
    "3. **Support Business Decision-Making:**  \n",
    "   Deliver **interpretable sentiment insights** to marketing teams and brand managers to:  \n",
    "   - Optimize campaigns  \n",
    "   - Address customer concerns  \n",
    "   - Enhance brand reputation  \n",
    "\n",
    "---\n",
    "\n",
    "### 1.4 Business Value\n",
    "\n",
    "An accurate sentiment analysis system delivers substantial value to decision-makers by enabling:\n",
    "\n",
    "- **Brand Monitoring**: Track customer feelings about the brand over time.  \n",
    "- **Marketing Optimization**: Pinpoint campaigns that drive positive engagement or negative feedback.  \n",
    "- **Customer Insights**: Uncover pain points or drivers of satisfaction.  \n",
    "- **Faster Decision-Making**: Provide near real-time feedback analysis.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1.5 Research Questions\n",
    "\n",
    "1. **How do customers feel about the company’s products or services**, based on sentiments expressed on Twitter?  \n",
    "2. **What key factors or topics drive positive and negative sentiments** toward the brand on Twitter?  \n",
    "3. **How can Twitter sentiment insights support business decisions**, such as marketing strategies, customer engagement, and brand reputation management?\n",
    "\n",
    "---\n",
    "\n",
    "### 1.6 Success Criteria\n",
    "\n",
    "The project's success will be measured by:\n",
    "\n",
    "1. **Actionable Insights**: The system delivers meaningful customer opinion trends on Twitter, supporting data-driven decisions.  \n",
    "2. **Brand Reputation Tracking**: Enables real-time monitoring of public sentiment, allowing timely responses to issues.  \n",
    "3. **Marketing and Engagement Impact**: Insights i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae509f43-27f9-47f9-a6f1-9d5416a95c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalid-abdulkadir/anaconda3/lib/python3.12/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package stopwords to /home/khalid-\n",
      "[nltk_data]     abdulkadir/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/khalid-\n",
      "[nltk_data]     abdulkadir/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/khalid-\n",
      "[nltk_data]     abdulkadir/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', module='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9558f76f-5ed6-4252-9d6b-3e688b65ae24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'judge-1377884607_tweet_product_company.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjudge-1377884607_tweet_product_company.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m data\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'judge-1377884607_tweet_product_company.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"judge-1377884607_tweet_product_company.csv\", encoding='latin1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde07827-bf9d-49f5-8fe2-dd4df545c5c8",
   "metadata": {},
   "source": [
    "# 📊 DATA UNDERSTANDING\n",
    "\n",
    "## 1. Data Collection\n",
    "- **Dataset Name:** judge-1377884607_tweet_product_company.csv  \n",
    "- **Source:** Twitter Sentiment Dataset (Data World)  \n",
    "- **Context:** Tweets labeled with emotions directed at brands or products.\n",
    "\n",
    "## 2. Data Description\n",
    "- **Number of Instances:** 9,093  \n",
    "- **Number of Features:** 3  \n",
    "  1. `tweet_text` – the content of the tweet  \n",
    "  2. `emotion_in_tweet_is_directed_at` – brand/product the emotion is directed at  \n",
    "  3. `is_there_an_emotion_directed_at_a_brand_or_product` – emotion label toward brand/product  \n",
    "\n",
    "## 3. Data Quality\n",
    "- **Encoding:** latin1  \n",
    "- **Duplicates:** 22  \n",
    "- **Missing Values:** Some possible in `emotion_in_tweet_is_directed_at`  \n",
    "- **Class Distribution (Target Variable):**\n",
    "  - No emotion toward brand or product → 5389  \n",
    "  - Positive emotion → 2978  \n",
    "  - Negative emotion → 570  \n",
    "  - I can't tell → 156  \n",
    "\n",
    "## 4. Initial Observations\n",
    "- Dataset is **imbalanced**, dominated by neutral tweets.  \n",
    "- Text data requires **preprocessing** (cleaning, tokenization, stopword removal).  \n",
    "- Will need **encoding** for categorical features and **balancing** before modeling.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8231266c-2683-4e08-b18a-c9a6c809a0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df703cf-4dd6-47c1-a8d0-83052ab9e768",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db25fff-ecb2-4cf1-bafc-8ac4939326f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85899feb-0a67-41b0-9cf0-438f1e160a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.emotion_in_tweet_is_directed_at.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d22193c-785f-423f-8236-8d61151458db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.is_there_an_emotion_directed_at_a_brand_or_product.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456d508f-142b-484c-a4aa-004afc1536a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee716dcb-28a5-4a0f-ab2c-549a09dffb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.emotion_in_tweet_is_directed_at.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f825b83c-dfd2-473d-89b1-64946b979718",
   "metadata": {},
   "source": [
    "# 🧹DATA PREPARATION\n",
    "\n",
    "## 1. Data Cleaning\n",
    "- **Filtered Data:** Kept only tweets with clear emotions (‘Positive emotion’, ‘Negative emotion’)\n",
    "- **Dropped Column:** `emotion_in_tweet_is_directed_at`\n",
    "- **Duplicates:** Removed previously (22 duplicates)\n",
    "- **Resulting Data Shape:** (number of records after filtering)\n",
    "\n",
    "## 2. Text Preprocessing\n",
    "Applied NLTK-based cleaning function:\n",
    "- Converted text to lowercase  \n",
    "- Removed URLs, mentions, hashtags, numbers, and punctuation  \n",
    "- Tokenized text and removed stopwords  \n",
    "- Lemmatized words using WordNetLemmatizer  \n",
    "- Created new column: `clean_text`\n",
    "\n",
    "## 3. Label Encoding\n",
    "Mapped sentiment labels:\n",
    "- Positive emotion → 1  \n",
    "- Negative emotion → 0  \n",
    "Created new column: `label`\n",
    "\n",
    "## 4. Data Splitting\n",
    "- Split data into **Train (80%)** and **Test (20%)** sets  \n",
    "- Used `stratify=y` to preserve class balance  \n",
    "\n",
    "## 5. Feature Engineering (Text to Numeric)\n",
    "- Used **TF-IDF Vectorizer** with:\n",
    "  - `max_features=5000`\n",
    "  - `ngram_range=(1,2)`  \n",
    "- Transformed text data into numeric feature matrix for model input.\n",
    "\n",
    "## 6. Handling Class Imbalance\n",
    "- Applied **SMOTE (Synthetic Minority Oversampling Technique)** on training data  \n",
    "- Balanced positive and negative samples for fair model training.\n",
    "\n",
    "---\n",
    "\n",
    "✅ *Output:* Cleaned, vectorized, and balanced dataset ready for the **Modeling Phase**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9916484-af17-46ae-ac56-d594a8fccbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values in each column\n",
    "missing_percentage = data.isnull().mean() * 100\n",
    "\n",
    "# Filter columns with more than 50% missing values\n",
    "missing_greater_than_50 = missing_percentage[missing_percentage > 50]\n",
    "\n",
    "# Display them\n",
    "missing_greater_than_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f005f46-85a8-4eb4-bfcb-d3769b42553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['emotion_in_tweet_is_directed_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7d500d-f46b-4ce9-95e7-6adb276fcb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941a7b7c-c929-4026-8a29-84236446d8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.is_there_an_emotion_directed_at_a_brand_or_product.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b953c971-7c68-4c88-b9ab-36db10bdf9e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b859bda2-3618-4ebd-8bcd-f2db93e21a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['tweet_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008f95b0-0c1c-4e41-9901-e11e421a5dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f8ee1-c10b-4d5b-a794-92782934d16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e4575e-dc5b-4676-afe3-dd6372debb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd6225-e511-4476-aeb0-6eebe75080f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.is_there_an_emotion_directed_at_a_brand_or_product.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f2d8bd-968d-46b6-b691-3d509fec5a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de9781d-16af-43fe-9d18-fb9f28bbeed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.is_there_an_emotion_directed_at_a_brand_or_product.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c247ea6-2a60-4684-84a5-25f5fe131526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "class TextCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        text = re.sub(r'#', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [t for t in tokens if t not in self.stop_words]\n",
    "        tokens = [self.lemmatizer.lemmatize(t) for t in tokens]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.apply(self.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3031407-92ce-402e-a6e2-5b48019acb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary target\n",
    "data['binary_target'] = data['is_there_an_emotion_directed_at_a_brand_or_product'].map({\n",
    "    'Positive emotion': 1,\n",
    "    'Negative emotion': 0\n",
    "})\n",
    "\n",
    "# Multiclass target (example mapping)\n",
    "data['multiclass_target'] = data['is_there_an_emotion_directed_at_a_brand_or_product'].map({\n",
    "    'Positive emotion': 'positive',\n",
    "    'Negative emotion': 'negative',\n",
    "    'No emotion toward brand or product': 'neutral',\n",
    "    'I can’t tell': 'uncertain'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ede2b1c-b7ac-485d-beac-bba0503a9c37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='multiclass_target', data=data, palette='coolwarm')\n",
    "plt.title('Overall Sentiment Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc361ed-93e3-447d-ae02-51edfb4a92ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate word clouds for each sentiment\n",
    "sentiments = ['positive', 'negative', 'neutral']\n",
    "\n",
    "for sentiment in sentiments:\n",
    "    text = \" \".join(data[data['multiclass_target'] == sentiment]['tweet_text'].astype(str))\n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "        width=800, \n",
    "        height=400, \n",
    "        background_color='white',\n",
    "        colormap='viridis'\n",
    "    ).generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Most Common Words in {sentiment.capitalize()} Tweets\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b039a9-8462-4307-9012-ef1a49de9fc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.multiclass_target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e626f-a8c6-4632-bdb6-a45546487efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keeping only rows that have binary labels\n",
    "bin_data = data[data['is_there_an_emotion_directed_at_a_brand_or_product'].isin(['Positive emotion', 'Negative emotion'])]\n",
    "\n",
    "# Droping any possible NaNs\n",
    "bin_data = bin_data.dropna(subset=['binary_target'])\n",
    "\n",
    "# target and feature\n",
    "X = bin_data['tweet_text']\n",
    "y = bin_data['binary_target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9143edf-f7ba-4870-8250-4b44c5514fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping Nans\n",
    "multi_data = data.dropna(subset=['multiclass_target'])\n",
    "\n",
    "#Features and target\n",
    "X_multi = multi_data['tweet_text']\n",
    "y_multi = multi_data['multiclass_target']\n",
    "\n",
    "# Train test split\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n",
    "    X_multi, y_multi, test_size=0.2, stratify=y_multi, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd17807f-0436-4ceb-a6e2-7371994d3b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pipeline = ImbPipeline([\n",
    "    ('cleaner', TextCleaner()),  \n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1,2))),  \n",
    "    ('smote', SMOTE(random_state=42)),  \n",
    "    ('model', LogisticRegression(max_iter=1000, random_state=42))  # placeholder model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804d17b3-07aa-42cf-8ddf-258028f728f4",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Base Pipeline for multi class\n",
    "base_pipeline_multi = ImbPipeline([\n",
    "    ('cleaner', TextCleaner()),\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1, 2))),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('model', LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edbe776-852e-4b1d-8b9a-8285dd13bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target labels for XGBOOST\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train_multi)\n",
    "y_test_enc = le.transform(y_test_multi)\n",
    "\n",
    "# Define the XGBoost pipeline\n",
    "pipeline_xgb_multi = ImbPipeline([\n",
    "    ('cleaner', TextCleaner()), \n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1, 2))),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('model', XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        objective='multi:softmax',\n",
    "        num_class=3,\n",
    "        eval_metric='mlogloss'\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffec6c9f-5166-4325-bdad-4f3051d1a6cc",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c77705-206b-4ae4-be5c-7c19e5db757d",
   "metadata": {},
   "source": [
    "### Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20795f03-f50f-45a8-b850-7e81f8ad4959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a Logistic Regression\n",
    "pipeline_lr = base_pipeline.set_params(model=LogisticRegression(max_iter=1000, random_state=42))\n",
    "pipeline_lr.fit(X_train, y_train)\n",
    "y_pred_lr = pipeline_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e69c9be-7535-45e3-92c1-deef87c2d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a Random Forest\n",
    "pipeline_rf = base_pipeline.set_params(model=RandomForestClassifier(n_estimators=200, random_state=42,class_weight='balanced'))\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "y_pred_rf = pipeline_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa43460-34cc-46f5-9639-485d3b7cc43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a XGBoost\n",
    "pipeline_xgb = base_pipeline.set_params(model=XGBClassifier(eval_metric='logloss', random_state=42))\n",
    "pipeline_xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = pipeline_xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f79e62-26bc-4dde-bfbe-e0e7c15451cd",
   "metadata": {},
   "source": [
    "# Multi Class Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e213b64e-d296-4b1c-ae28-7568a3f81115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "pipeline_lr_multi = base_pipeline_multi.set_params(\n",
    "    model=LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial', class_weight='balanced')\n",
    ")\n",
    "pipeline_lr_multi.fit(X_train_multi, y_train_multi)\n",
    "y_pred_lr_multi = pipeline_lr_multi.predict(y_test_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e1b06-fec5-4fbd-b901-5eb56936fa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "pipeline_rf_multi = base_pipeline_multi.set_params(\n",
    "    model=RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced')\n",
    ")\n",
    "pipeline_rf_multi.fit(X_train_multi, y_train_multi)\n",
    "y_pred_rf_multi = pipeline_rf_multi.predict(y_test_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1df9f6b-1754-4aae-ad90-0b9d709769ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "pipeline_xgb_multi.fit(X_train_multi, y_train_enc)\n",
    "y_pred_xgb_enc = pipeline_xgb_multi.predict(X_test_multi)\n",
    "# Decode numeric predictions back to string labels\n",
    "y_pred_xgb_multi = le.inverse_transform(y_pred_xgb_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d90edd-17ec-4126-9ff7-34ac839616d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Evaluations\n",
    "def evaluate_model(name, y_test, y_pred):\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n Results for {name}\")\n",
    "    print(f\" Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])\n",
    "    disp.plot(cmap='Blues', values_format='d')\n",
    "    plt.title(f\"Confusion Matrix - {name} (Acc: {acc:.2f})\")\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "evaluate_model(\"Logistic Regression\", y_test, y_pred_lr)\n",
    "evaluate_model(\"Random Forest\", y_test, y_pred_rf)\n",
    "evaluate_model(\"XGBoost\", y_test, y_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b140b69-faf5-41b9-a799-29d1dcbb35d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Class Evaluation Function\n",
    "def evaluate_model(name, y_test, y_pred):\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\n Results for {name}\")\n",
    "    print(f\" Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=sorted(y_test.unique()))\n",
    "    disp.plot(cmap='Blues', values_format='d')\n",
    "    plt.title(f\"Confusion Matrix - {name} (Acc: {acc:.2f})\")\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "# Evaluating both models for multi class\n",
    "evaluate_model(\"Logistic Regression (Multiclass)\", y_test_multi, y_pred_lr_multi)\n",
    "evaluate_model(\"Random Forest (Multiclass)\", y_test_multi, y_pred_rf_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97c6b4e-0da2-4e2d-9105-3fee083e4de8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluatioions of XGBOOST\n",
    "acc = accuracy_score(y_test_multi, y_pred_xgb_multi)\n",
    "print(f\"\\n Results for XGBoost (Multiclass)\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(classification_report(y_test_multi, y_pred_xgb_multi))\n",
    "\n",
    "cm = confusion_matrix(y_test_multi, y_pred_xgb_multi, labels=le.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, displayimport joblib\n",
    "_labels=le.classes_)\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title(f\"Confusion Matrix - XGBoost (Acc: {acc:.2f})\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f63a1b-026b-47ca-bcc5-b493c25b2f58",
   "metadata": {},
   "source": [
    "## 🧩 Conclusions\n",
    "\n",
    "1. **Overall Sentiment:**  \n",
    "   Analysis of the Twitter data shows that the majority of tweets are **neutral**, followed by **positive** and then **negative** sentiments.  \n",
    "   This indicates that most users discuss the company’s products and services in an informative or neutral way rather than expressing strong emotions.  \n",
    "\n",
    "2. **Customer Perception:**  \n",
    "   The word cloud for positive tweets highlights common terms such as **“SXSW”**, **“Google”**, **“Apple”**, **“iPad”**, and **“iPhone.”**  \n",
    "   These frequent mentions suggest that customers express excitement and satisfaction around **product launches**, **technology events**, and **brand innovations** — reflecting a generally favorable perception of the company’s offerings.  \n",
    "\n",
    "3. **Model Performance:**  \n",
    "   The **Logistic Regression model** achieved an **accuracy of 86%**, showing strong performance in classifying tweet sentiments.  \n",
    "   The confusion matrix indicates that the model identifies **positive tweets** more accurately than **negative ones**, meaning it could still improve in detecting subtle negative tones or mixed emotions.\n",
    "\n",
    "\n",
    "## 💡 Recommendations\n",
    "\n",
    "1. **Strengthen Positive Engagement:**  \n",
    "   Since positive sentiments are strongly linked to specific events and products, the company should continue leveraging **major tech events** (like SXSW) and **new product launches** to maintain excitement and engagement among users.\n",
    "\n",
    "2. **Investigate Negative Sentiments:**  \n",
    "   Although negative tweets are fewer, they provide valuable feedback.  \n",
    "   The company should perform **deeper analysis** (e.g., topic modeling or keyword extraction) on these tweets to identify recurring issues such as product dissatisfaction or service complaints and address them promptly.\n",
    "\n",
    "3. **Enhance the Sentiment Model:**  \n",
    "   - Experiment with more advanced models such as **Random Forest**, **XGBoost**, or **BERT-based classifiers** to capture context and sarcasm better.  \n",
    "   - Include more **balanced training data** to improve detection of underrepresented sentiments (especially negative).  \n",
    "   - Fine-tune text preprocessing to handle **slang, emojis, and hashtags**, which often carry emotional cues.\n",
    "\n",
    "4. **Continuous Monitoring:**  \n",
    "   Set up a **real-time sentiment monitoring system** to track changes in public perception over time.  \n",
    "   This will help evaluate how new products, marketing campaigns, or company decisions affect customer attitudes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0359f7-1e61-4dce-994c-476b9318b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "\n",
    "# # Save the trained model\n",
    "# joblib.dump(pipeline_lr_multi, 'sentiment_models.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce26c8d-602e-4ab9-9119-f9d413de63f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
